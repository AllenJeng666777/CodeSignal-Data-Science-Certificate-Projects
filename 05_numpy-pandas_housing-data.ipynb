{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223472e7",
   "metadata": {},
   "source": [
    "# Exploring the California Housing Dataset: An Introduction to Dataset Characteristics and Basic Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03657f",
   "metadata": {},
   "source": [
    "Great job on completing the lesson, Voyager! Are you ready to kick off the practice session? ðŸš€\n",
    "\n",
    "You've just moved to California. As a data scientist, you're curious about the housing market. Would you be able to explore the basics of your new town using the California Housing dataset? Do you know the average number of rooms (avg_rooms) and the median income (median_income) in your neighborhood?\n",
    "\n",
    "Feel your way around your new environment by executing the provided starter code. It will guide you through the journey of exploring your new surroundings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db3d29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe:  (20640, 9)\n",
      "\n",
      "Statistical Summary for the dataset:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude  MedHouseValue  \n",
      "count  20640.000000  20640.000000  20640.000000   20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704       2.068558  \n",
      "std       10.386050      2.135952      2.003532       1.153956  \n",
      "min        0.692308     32.540000   -124.350000       0.149990  \n",
      "25%        2.429741     33.930000   -121.800000       1.196000  \n",
      "50%        2.818116     34.260000   -118.490000       1.797000  \n",
      "75%        3.282261     37.710000   -118.010000       2.647250  \n",
      "max     1243.333333     41.950000   -114.310000       5.000010  \n",
      "\n",
      "Checking for missing values in the dataset:\n",
      "MedInc           0\n",
      "HouseAge         0\n",
      "AveRooms         0\n",
      "AveBedrms        0\n",
      "Population       0\n",
      "AveOccup         0\n",
      "Latitude         0\n",
      "Longitude        0\n",
      "MedHouseValue    0\n",
      "dtype: int64\n",
      "\n",
      "First few rows of the dataset:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MedHouseValue  \n",
      "0    -122.23          4.526  \n",
      "1    -122.22          3.585  \n",
      "2    -122.24          3.521  \n",
      "3    -122.25          3.413  \n",
      "4    -122.25          3.422  \n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the dataset\n",
    "dataset = fetch_california_housing()\n",
    "\n",
    "# Create a dataframe for dataset\n",
    "df = pd.DataFrame(data = dataset.data, columns = dataset.feature_names)\n",
    "\n",
    "# Add the target values to the dataframe\n",
    "df[\"MedHouseValue\"] = dataset.target\n",
    "\n",
    "# Print the size of dataframe\n",
    "print(\"Size of the dataframe: \", df.shape)\n",
    "\n",
    "# Get the statistical summary for the dataset\n",
    "print(\"\\nStatistical Summary for the dataset:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Print the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5661b",
   "metadata": {},
   "source": [
    "Fantastic work, Explorer! You've successfully examined the California housing dataset. Now, it's time to delve deeper into our exploration and analyze this dataset more meticulously.\n",
    "\n",
    "Specifically, could you create a new column PopHouseValue? This column should represent the total population per total household value for each block. Try implementing this in the DataFrame.\n",
    "\n",
    "May the force of data be with you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e49e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MedHouseValue  PopHouseValue  \n",
      "0    -122.23          4.526      71.144498  \n",
      "1    -122.22          3.585     669.735007  \n",
      "2    -122.24          3.521     140.869071  \n",
      "3    -122.25          3.413     163.492529  \n",
      "4    -122.25          3.422     165.108124  \n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n",
    "                  columns=data['feature_names'] + ['MedHouseValue'])\n",
    "\n",
    "# TODO: Add a new column 'PopHouseValue' describing 'Population' divided by 'MedHouseValue'\n",
    "df['PopHouseValue'] = df['Population'] / df['MedHouseValue']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30c4e4",
   "metadata": {},
   "source": [
    "# Matrix Operations in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c61c84",
   "metadata": {},
   "source": [
    "Woof! You've blasted through some advanced Numpy concepts so far. Ready to put your newfound skills to the test?\n",
    "\n",
    "Let's imagine you've got several dataset features, and you want to combine them into one matrix (known as feature combination in machine learning). Additionally, you'd like to normalize these features to ensure all values fall within the same range. Moreover, let's compute these features' weighted sum based on predefined weight parameters â€“ a necessary step for many machine learning models.\n",
    "\n",
    "Pedal to the metal, Space Explorer! Run the starter code to see it all in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1493a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features combined:\n",
      " [[123 321 135 531 999]\n",
      " [456 654 246 642 888]\n",
      " [789 987 357 753 777]]\n",
      "Normalized data:\n",
      " [[0.04936571 0.12883247 0.05418188 0.21311539 0.40094591]\n",
      " [0.18301435 0.26248111 0.09873143 0.25766494 0.35639636]\n",
      " [0.31666299 0.39612974 0.14328097 0.30221448 0.31184682]]\n",
      "Weighted sum of features:\n",
      " [0.18614486 0.25296918 0.31979349]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume that we have five features for the California Housing dataset\n",
    "feature_1 = np.array([[123], [456], [789]])\n",
    "feature_2 = np.array([[321], [654], [987]])\n",
    "feature_3 = np.array([[135], [246], [357]])\n",
    "feature_4 = np.array([[531], [642], [753]])\n",
    "feature_5 = np.array([[999], [888], [777]])\n",
    "\n",
    "# Concatenate all five features into one matrix\n",
    "all_features = np.hstack((feature_1, feature_2, feature_3, feature_4, feature_5))\n",
    "print(\"Features combined:\\n\", all_features)\n",
    "\n",
    "# Normalize all data to ensure consistent scale\n",
    "normalized_all_features = all_features / np.linalg.norm(all_features)\n",
    "print(\"Normalized data:\\n\", normalized_all_features)\n",
    "\n",
    "# Simulate the weights of features (5 features now in total)\n",
    "weights = np.array([0.2, 0.3, 0.1, 0.15, 0.25])\n",
    "\n",
    "# Calculate the weighted sum of all features\n",
    "weighted_sum_features = np.dot(normalized_all_features, weights)\n",
    "print(\"Weighted sum of features:\\n\", weighted_sum_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8a12c",
   "metadata": {},
   "source": [
    "Great job fetching these matrices! Ready for some fun with a twist this time?\n",
    "\n",
    "Suppose you are working with an image processing task, and your image pixel intensities range from 0 to 255. But in your machine learning model, you expect pixel intensities normalized between 0 and 1. Can you adjust the existing starter code to normalize the matrix_a in the range between 0 and 1?\n",
    "\n",
    "Instead of finding its L2 norm and dividing, try scaling them according to their max and min values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7493adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[247  97  20]\n",
      " [253  22  48]\n",
      " [ 56 231 101]]\n",
      "\n",
      "Matrix A normalized to [0, 1]:\n",
      " [[0.97424893 0.3304721  0.        ]\n",
      " [1.         0.00858369 0.12017167]\n",
      " [0.15450644 0.9055794  0.34763948]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create two random 3x3 matrices using `np.random.randint`\n",
    "matrix_a = np.random.randint(255, size=(3, 3))\n",
    "matrix_b = np.random.randint(255, size=(3, 3))\n",
    "\n",
    "print(\"Matrix A:\\n\", matrix_a)\n",
    "\n",
    "# Normalize Matrix A by dividing each element by its Norm (L2 Norm typically used)\n",
    "# --- Minâ€“max normalization to [0, 1] ---------------------------------\n",
    "min_val = matrix_a.min()\n",
    "max_val = matrix_a.max()\n",
    "\n",
    "# Avoid division-by-zero if all pixels are identical\n",
    "range_val = max_val - min_val if max_val != min_val else 1\n",
    "\n",
    "norm_matrix_a = (matrix_a - min_val) / range_val\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nMatrix A normalized to [0, 1]:\\n\", norm_matrix_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b09094",
   "metadata": {},
   "source": [
    "High five, space explorer! Now that you've been delving into matrix operations and their applications in Machine Learning, you are faced with an image processing task. The pixel intensities need to be normalized in the range from 0 to 1, which went smoothly.\n",
    "\n",
    "Still, the computation of the weighted sum, after transposing the matrix, gives unexpected results. Can you figure out what went wrong and how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa4fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Pixels:\n",
      " [[175 200 225]\n",
      " [190 250 220]\n",
      " [205 255 230]]\n",
      "Normalized Image Pixels:\n",
      " [[0.     0.3125 0.625 ]\n",
      " [0.1875 0.9375 0.5625]\n",
      " [0.375  1.     0.6875]]\n",
      "Transposed Normalized Pixel Matrix:\n",
      " [[0.     0.1875 0.375 ]\n",
      " [0.3125 0.9375 1.    ]\n",
      " [0.625  0.5625 0.6875]]\n",
      "Weighted Sum:\n",
      " [0.1875  0.76875 0.61875]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume these are image pixel values\n",
    "pixels_1 = np.array([175, 200, 225])\n",
    "pixels_2 = np.array([190, 250, 220])\n",
    "pixels_3 = np.array([205, 255, 230])\n",
    "\n",
    "# Combine the pixel groups into one matrix\n",
    "image_pixels = np.vstack((pixels_1, pixels_2, pixels_3))\n",
    "print(\"Image Pixels:\\n\", image_pixels)\n",
    "\n",
    "# Normalize the pixel intensities to range [0, 1]\n",
    "normalized_image_pixels = (image_pixels - np.min(image_pixels)) / (np.max(image_pixels) - np.min(image_pixels))\n",
    "print(\"Normalized Image Pixels:\\n\", normalized_image_pixels)\n",
    "\n",
    "# Transpose the normalized pixel matrix\n",
    "transposed_pixels = normalized_image_pixels.T\n",
    "print(\"Transposed Normalized Pixel Matrix:\\n\", transposed_pixels)\n",
    "\n",
    "# Weights for the pixels' features\n",
    "weights = np.array([0.3, 0.4, 0.3])\n",
    "\n",
    "# Compute the weighted sum of the transposed pixel matrix\n",
    "weighted_sum = np.dot(transposed_pixels, weights)\n",
    "print(\"Weighted Sum:\\n\", weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d02ff",
   "metadata": {},
   "source": [
    "Well done, Stellar Navigator! Proud of you! You've whizzed through these advanced Numpy operations. Now, let's take our learning up a notch! Time to prove your mettle by writing code from scratch.\n",
    "\n",
    "Ready? For this task, given two \n",
    "3\n",
    "Ã—\n",
    "3\n",
    "3Ã—3 matrices, normalize the first of them, take its transpose, calculate the dot product with a given weights array, and calculate the pseudo-inverse of the first matrix. Yep, that's a lot, but I know you can handle it! Remember, the devilâ€™s in the detail!\n",
    "\n",
    "Time to board the spaceship!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600ea9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized matrix_A:\n",
      " [[0.         0.         0.14285714]\n",
      " [0.28571429 0.42857143 0.57142857]\n",
      " [0.71428571 0.85714286 1.        ]]\n",
      "Transpose matrix_A:\n",
      " [[0.         0.28571429 0.71428571]\n",
      " [0.         0.42857143 0.85714286]\n",
      " [0.14285714 0.57142857 1.        ]]\n",
      "Weighted Sum:\n",
      " [0.28571429 0.38571429 0.52857143]\n",
      "Weighted Sum:\n",
      " [-0.5 -0.2  0.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initializing two 3x3 matrices using numpy array\n",
    "matrix_A = np.array([[3, 3, 4], [5, 6, 7], [8, 9, 10]])\n",
    "matrix_B = np.array([[12, 13, 14], [14, 16, 17], [19, 20, 21]])\n",
    "\n",
    "# TODO: Normalize matrix_A in the range [0,1] using min-max normalization\n",
    "normalized_matrix_A = (matrix_A - np.min(matrix_A)) / (np.max(matrix_A) - np.min(matrix_A))\n",
    "print(\"Normalized matrix_A:\\n\", normalized_matrix_A)\n",
    "# TODO: Take the transpose of the normalized matrix_A\n",
    "transposed_pixels = normalized_matrix_A.T\n",
    "print(\"Transpose matrix_A:\\n\", transposed_pixels)\n",
    "# Assume these are the weights for features in matrix_A\n",
    "weights = np.array([0.3, 0.5, 0.2])\n",
    "\n",
    "# TODO: Compute the dot product of the transposed normalized matrix and weights\n",
    "weighted_sum = np.dot(transposed_pixels, weights)\n",
    "print(\"Weighted Sum:\\n\", weighted_sum)\n",
    "# TODO: Calculate the inverse matrix for `matrix_A` and calculate their product - it should result in the identity matrix\n",
    "inverse_matrix_A = np.linalg.inv(matrix_A)\n",
    "weighted_inverse_sum = np.dot(inverse_matrix_A, weights)\n",
    "print(\"Weighted Sum:\\n\", weighted_inverse_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7c3db",
   "metadata": {},
   "source": [
    "# Introduction to Mastering Pandas: Advanced Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a1615",
   "metadata": {},
   "source": [
    "Alright, Galactic Pioneer, welcome to your practice session. We're going to delve into a critical question here: In our California dataset, what's the median income of households with varying average numbers of rooms per household?\n",
    "\n",
    "You'll need to calculate a new feature, RoomPerHousehold, then group the data according to this, and finally get the median income for each group in the DataFrame.\n",
    "\n",
    "Ready to see this in action? Hit the Run button to dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bacc3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoomPerHousehold\n",
      "0.002547     10.2264\n",
      "0.008576      5.5179\n",
      "0.018065      4.2639\n",
      "0.035955      6.1359\n",
      "0.061605      4.2391\n",
      "              ...   \n",
      "33.843373     1.6154\n",
      "34.214286     2.6250\n",
      "41.333333     2.5893\n",
      "52.033333     1.8750\n",
      "55.222222     4.6250\n",
      "Length: 20352, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Add a 'RoomPerHousehold' feature to the dataframe\n",
    "housing_df['RoomPerHousehold'] = housing_df['AveRooms'] / housing_df['AveOccup']\n",
    "\n",
    "# Group by 'RoomPerHousehold' category and get the median of 'MedInc' for each group\n",
    "med_house_val = housing_df.groupby('RoomPerHousehold').apply(lambda x: x['MedInc'].median())\n",
    "\n",
    "# Print the result\n",
    "print(med_house_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ab074",
   "metadata": {},
   "source": [
    "Up high, the view is fantastic! You've just mastered creating new derived columns and applying operations on them.\n",
    "\n",
    "In your previous task, you grouped data by income, but what if you grouped it by the average room categories? Could you change the code to calculate median income (MedInc) for different room categories (Room_cat)?\n",
    "\n",
    "Satellites are ready. Let's launch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1709a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room_cat\n",
      "Low              1.5553\n",
      "Below Average    2.3279\n",
      "Average          3.3566\n",
      "Above Average    5.3548\n",
      "High             6.4744\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Define income category\n",
    "housing_df['Room_cat'] = pd.cut(housing_df['AveRooms'],\n",
    "                               bins=[0., 2.0, 4.0, 6.0, 8.0, np.inf],\n",
    "                               labels=['Low', 'Below Average', 'Average', 'Above Average', 'High'])\n",
    "\n",
    "# TODO: Group by room category and calculate average median income instead\n",
    "average_value = housing_df.groupby('Room_cat').apply(lambda x: x['MedInc'].median())\n",
    "\n",
    "# Print the result\n",
    "print(average_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0053cd35",
   "metadata": {},
   "source": [
    "Well done, champ! Now, can you take a breather and find out if there's any correlation between the maximum median income and the average number of bedrooms per unit?\n",
    "\n",
    "The average number of bedrooms per unit is the average number of bedrooms (AveBedrms) divided by the 'AveOccup' column value. The maximum median income is the highest median income value within each category of average bedrooms per unit would be beneficial.\n",
    "\n",
    "Don't hesitate to use a bar chart to visualize your results. Looks like the code below tries to do just that. But wait, there seems to be an issue with the code!\n",
    "\n",
    "Reckon, you can figure out what's amiss and get it back on track. Go for it, Space Voyager!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ba50f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BedroomsCat\n",
      "Small     15.0001\n",
      "Medium    15.0001\n",
      "Large      7.6281\n",
      "XLarge    10.2948\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Define a new feature: 'AvgBedroomsPerUnit', that calculates the average bedrooms per unit\n",
    "housing_df['AvgBedroomsPerUnit'] = housing_df['AveBedrms'] / housing_df['AveOccup']\n",
    "\n",
    "# Define room size categories using the 'AvgBedroomsPerUnit' feature\n",
    "housing_df['BedroomsCat'] = pd.cut(housing_df['AvgBedroomsPerUnit'],\n",
    "                               bins=[0., 0.5, 1.0, 1.5, np.inf],\n",
    "                               labels=['Small', 'Medium', 'Large', 'XLarge'])\n",
    "\n",
    "max_income = housing_df.groupby('BedroomsCat').apply(lambda x: x['MedInc'].max())\n",
    "\n",
    "print(max_income)\n",
    "\n",
    "# Plot the result as a bar chart with the title 'Max Income by Bedrooms Category'\n",
    "max_income.plot(kind='bar', title='Max Income by Bedrooms Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640f550",
   "metadata": {},
   "source": [
    "Spectacular work, data astronaut! Now, let's push it to the limit. Imagine you work for a city's population analytics bureau, and they're interested in how household sizes vary across areas with different average room sizes.\n",
    "\n",
    "In the starter code, can you devise a way to categorize the room sizes and group the data by these categories? After that, derive the average population for each category. Sounds challenging? You've got this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d981c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoomSizeCat\n",
      "Small     1552.789800\n",
      "Medium    1397.132786\n",
      "Large     1100.969565\n",
      "XLarge            NaN\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "# Define a new feature: AveRooms/AveBedrms, that calculates average room size in terms of bedroom\n",
    "housing_df['RoomSize'] = housing_df['AveRooms'] / housing_df['AveBedrms']\n",
    "\n",
    "# TODO: Define room size categories using the 'RoomSize' feature.\n",
    "housing_df['RoomSizeCat'] = pd.cut(housing_df['RoomSize'],\n",
    "    bins=[0,4,8,10,np.inf], labels=['Small', 'Medium', 'Large', 'XLarge'])\n",
    "\n",
    "# Group by RoomSizeCat and calculate the average population\n",
    "average_population = housing_df.groupby('RoomSizeCat').apply(lambda x: x['Population'].mean())\n",
    "\n",
    "# Print the result\n",
    "print(average_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff7dba",
   "metadata": {},
   "source": [
    "Magnificent work, data scientist! You're about to complete your journey into advanced Pandas functions for this lesson!\n",
    "\n",
    "Now, let's apply all the knowledge we've gained. Imagine you are working with the California Housing dataset, and you'd like to answer this question: How does the average number of bedrooms vary across various age categories of houses?\n",
    "\n",
    "Your task is to categorize the ages, group the data by these categories, and compute the average number of bedrooms for each category. Use the instructions in the starter code to guide your implementation!\n",
    "\n",
    "Ready to finish this adventure strong? Let's leap into the data galaxy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc8f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    41.0\n",
      "1    21.0\n",
      "2    52.0\n",
      "3    52.0\n",
      "4    52.0\n",
      "Name: HouseAge, dtype: float64\n",
      "Age_cat\n",
      "Very New    1.153912\n",
      "New         1.149966\n",
      "Medium      1.096420\n",
      "Old         1.062850\n",
      "Very Old    1.057503\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import matplotlib.pyplot as plt\n",
    "# Fetch the dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "housing_df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "print(housing_df['HouseAge'].head(5))\n",
    "# TODO: Create categories for the 'HouseAge' feature. You can label them as 'Very New', 'New', 'Medium', 'Old', and 'Very Old'.\n",
    "bins = [1, 10, 20, 30, 40, np.inf]\n",
    "labels = ['Very New', 'New', 'Medium', 'Old', 'Very Old']\n",
    "housing_df['Age_cat'] = pd.cut(housing_df['HouseAge'], labels=labels, bins=bins)\n",
    "\n",
    "# TODO: Group the data by the new 'Age_cat' category and calculate the average number of bedrooms for each category.\n",
    "average_age = housing_df.groupby('Age_cat').apply(lambda x: x['AveBedrms'].mean())\n",
    "# TODO: Print the results\n",
    "print(average_age)\n",
    "# TODO: Plot the result as a bar chart\n",
    "average_age.plot(kind='bar', title='Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4291879",
   "metadata": {},
   "source": [
    "# Introduction to Code Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606c2eb",
   "metadata": {},
   "source": [
    "Alright, stargazer, ready to kickstart your practice session?\n",
    "\n",
    "Imagine you're a data scientist at a large tech company working with a massive data array. Your Python code uses the built-in sum() function to calculate the sum of numbers, but you noticed that it takes a huge amount of time to execute for larger arrays. You've heard that Numpy's built-in functions could perform faster, but you're not sure if this is true.\n",
    "\n",
    "You've decided to test it yourself to see the real difference. All you need to do now is just press the Run button to see which way is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e727751c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Sum: 25019.808781005147, Time Taken: 0 days 00:00:00.003989\n",
      "Numpy Sum: 25019.808781004987, Time Taken: 0 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a large array\n",
    "large_array = np.random.rand(50000)\n",
    "\n",
    "# Compute the sum using Python's built-in function, time it, and print the result\n",
    "python_start_time = pd.Timestamp.now()\n",
    "python_sum = sum(large_array)\n",
    "python_end_time = pd.Timestamp.now()\n",
    "print(f\"Python Sum: {python_sum}, Time Taken: {python_end_time - python_start_time}\")\n",
    "\n",
    "# Compute the sum using Numpy's built-in function, time it, and print the result\n",
    "numpy_start_time = pd.Timestamp.now()\n",
    "numpy_sum = np.sum(large_array)\n",
    "numpy_end_time = pd.Timestamp.now()\n",
    "print(f\"Numpy Sum: {numpy_sum}, Time Taken: {numpy_end_time - numpy_start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c302f",
   "metadata": {},
   "source": [
    "Well done on running your first practice session! Now, you're progressing like a full-throttle space rocket! Let's move beyond running the code â€” we're tweaking now.\n",
    "\n",
    "The provided starter code measures the memory usage before and after data type optimization and the time taken for optimization. However, as a data scientist, you would also like to see the percentage of the memory saved. By understanding the percentage of memory saved, we can better appreciate the importance of optimization techniques in saving memory.\n",
    "\n",
    "Let's roar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984d8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad0a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory usage before optimization:  1.2598876953125 MB\n",
      "The memory usage after optimization:  0.570953369140625 MB\n",
      "The memory reduced:  0.688934326171875 MB\n",
      "Time taken for optimization:  0.0  seconds\n",
      "Percentage of memory saved: 54.68%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the California Housing dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n",
    "\n",
    "# Function to calculate memory usage\n",
    "def get_memory_usage(df):\n",
    "    bytes = df.memory_usage(deep=True).sum()\n",
    "    mb = bytes / 1024**2  # convert bytes to megabytes\n",
    "    return mb\n",
    "\n",
    "# Measure initial memory usage\n",
    "start_mem = get_memory_usage(df)\n",
    "\n",
    "# Optimize DataFrame memory usage by changing data types\n",
    "start_time = time.time()\n",
    "df['AveBedrms'] = pd.to_numeric(df['AveBedrms'], downcast='float')\n",
    "df['AveRooms'] = pd.to_numeric(df['AveRooms'], downcast='float')\n",
    "df['AveOccup'] = pd.to_numeric(df['AveOccup'], downcast='float')\n",
    "df['Latitude'] = pd.to_numeric(df['Latitude'], downcast='float')\n",
    "df['Longitude'] = pd.to_numeric(df['Longitude'], downcast='float')\n",
    "df['MedInc'] = pd.to_numeric(df['MedInc'], downcast='float')\n",
    "df['Population'] = pd.to_numeric(df['Population'], downcast='integer')\n",
    "df['HouseAge'] = pd.to_numeric(df['HouseAge'], downcast='integer')\n",
    "\n",
    "# After memory optimization\n",
    "end_memory = get_memory_usage(df)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"The memory usage before optimization: \", start_mem, \"MB\")\n",
    "print(\"The memory usage after optimization: \", end_memory, \"MB\")\n",
    "print(\"The memory reduced: \", start_mem - end_memory, \"MB\")\n",
    "print('Time taken for optimization: ', end_time - start_time, ' seconds')\n",
    "print(\"Percentage of memory saved: {:.2f}%\".format(100 * (start_mem - end_memory) / start_mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c377c5b",
   "metadata": {},
   "source": [
    "Bravo on your progress! Being confident in running and tweaking code is fantastic, but now it's time to unleash the power of your debugging skills.\n",
    "\n",
    "Given a bug in the code that's supposed to optimize memory usage for the California Housing dataset, your task is to find and fix it to ensure proper memory optimization. The hitch is it's not reflecting any memory reduction after the optimization.\n",
    "\n",
    "Immerse yourself in the process, and remember, the bug you're looking for might be small, but every step in learning counts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e839af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory usage before optimization:  1.2598876953125 MB\n",
      "The memory usage after optimization:  0.6300048828125 MB\n",
      "The memory reduced:  0.6298828125 MB\n",
      "Time taken for optimization:  0.003116130828857422  seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the California Housing dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n",
    "\n",
    "# Function to calculate memory usage\n",
    "def get_memory_usage(df):\n",
    "    bytes = df.memory_usage(deep=True).sum()\n",
    "    mb = bytes / 1024**2  # convert bytes to megabytes\n",
    "    return mb\n",
    "\n",
    "# Measure initial memory usage\n",
    "start_mem = get_memory_usage(df)\n",
    "\n",
    "# Optimize DataFrame memory usage by using method chaining for all column data type changes\n",
    "start_time = time.time()\n",
    "df = df.astype({\n",
    "    'AveBedrms': 'float32',\n",
    "    'AveRooms': 'float32',\n",
    "    'AveOccup': 'float32',\n",
    "    'Latitude': 'float32',\n",
    "    'Longitude': 'float32',\n",
    "    'MedInc': 'float32',\n",
    "    'Population': 'int32',\n",
    "    'HouseAge': 'int32'}, \n",
    "    copy=False)\n",
    "\n",
    "# After memory optimization\n",
    "end_memory = get_memory_usage(df)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"The memory usage before optimization: \", start_mem, \"MB\")\n",
    "print(\"The memory usage after optimization: \", end_memory, \"MB\")\n",
    "print(\"The memory reduced: \", start_mem - end_memory, \"MB\")\n",
    "print('Time taken for optimization: ', end_time - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815428f",
   "metadata": {},
   "source": [
    "Great job optimizing the DataFrame, data astronaut! Now, I have a new challenge for you. Let's dive further into the magic of optimization.\n",
    "\n",
    "Can you convert the 'Population' feature from a float to an integer data type using the astype method? Also, convert the 'HouseAge' feature to a categorical data type using the pd.cut method. This will split HouseAge into five equal-width bins labeled 0 to 4. Don't forget to save these adjustments in the DataFrame.\n",
    "\n",
    "Let's see those optimization skills in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43612a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 1.26 MB\n",
      "Final memory usage: 1.04 MB\n",
      "Memory saved by optimizing data types: 17.17%\n",
      "0    1823\n",
      "1    4916\n",
      "2    4864\n",
      "3    5455\n",
      "4    3582\n",
      "Name: HouseAge, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n",
    "\n",
    "# Function to calculate memory usage\n",
    "def calculate_memory(df):\n",
    "    bytes = df.memory_usage(deep=True).sum()\n",
    "    return bytes / 1024**2  # convert bytes to MB\n",
    "\n",
    "# Record initial memory usage\n",
    "init_memory = calculate_memory(df)\n",
    "\n",
    "# Convert 'Population' to integer\n",
    "df['Population'] = df['Population'].astype('int32')\n",
    "\n",
    "# Convert 'HouseAge' to 5 equal-width bins labeled 0â€“4 as category\n",
    "df['HouseAge'] = pd.cut(df['HouseAge'], bins=5, labels=[0, 1, 2, 3, 4]).astype('category')\n",
    "\n",
    "# Record final memory usage\n",
    "final_memory = calculate_memory(df)\n",
    "memory_reduction = 100 * (init_memory - final_memory) / init_memory\n",
    "\n",
    "# Output\n",
    "print(f\"Initial memory usage: {init_memory:.2f} MB\")\n",
    "print(f\"Final memory usage: {final_memory:.2f} MB\")\n",
    "print(f\"Memory saved by optimizing data types: {memory_reduction:.2f}%\")\n",
    "print(df['HouseAge'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c59379",
   "metadata": {},
   "source": [
    "Good job on your progress, Galaxy Explorer! You're about to take the final step in this leg of our journey: reinforcement. We're talking about rolling our sleeves and writing the entire implementation from scratch.\n",
    "\n",
    "With the California Housing dataset, you're to accomplish the following task: Optimize the memory usage by transforming a feature in the dataset. Your task is to take a MedInc feature of the dataset, which stands for the Median Income of households within a block of houses, and try to optimize the memory usage by transforming it.\n",
    "\n",
    "Follow the commented-out steps in the starter code, and remember what we learned about Numpy and Pandas optimization.\n",
    "\n",
    "Do you think you can rise to this challenge, comet chaser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bc7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 1.2599 MB\n",
      "Final memory usage after optimization: 1.1223 MB\n",
      "Memory reduced: 0.1376 MB (10.92%)\n",
      "Time taken for optimization: 0.0044 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import time\n",
    "\n",
    "# Load the California Housing Dataset\n",
    "california = datasets.fetch_california_housing()\n",
    "df = pd.DataFrame(data=california.data, columns=california.feature_names)\n",
    "\n",
    "# Function to calculate memory usage in MB\n",
    "def get_memory_usage(df):\n",
    "    bytes = df.memory_usage(deep=True).sum()\n",
    "    return bytes / 1024**2  # convert bytes to MB\n",
    "\n",
    "# Grab initial memory usage\n",
    "initial_memory = get_memory_usage(df)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Perform log transformation on 'MedInc'\n",
    "df['MedInc'] = np.log(df['MedInc'])\n",
    "\n",
    "# Step 2: Cut 'MedInc' into 5 equal-sized bins and label them 0â€“4\n",
    "df['MedInc'] = pd.cut(df['MedInc'], bins=5, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "# Step 3: Convert 'MedInc' to categorical type\n",
    "df['MedInc'] = df['MedInc'].astype('category')\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute memory usage after tuning\n",
    "final_memory = get_memory_usage(df)\n",
    "\n",
    "# Compute memory reduction percentage\n",
    "memory_saved = initial_memory - final_memory\n",
    "memory_reduction_percent = (memory_saved / initial_memory) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Initial memory usage: {initial_memory:.4f} MB\")\n",
    "print(f\"Final memory usage after optimization: {final_memory:.4f} MB\")\n",
    "print(f\"Memory reduced: {memory_saved:.4f} MB ({memory_reduction_percent:.2f}%)\")\n",
    "print(f\"Time taken for optimization: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66cbab7",
   "metadata": {},
   "source": [
    "# Expanding Horizons: Applications of Numpy and Pandas in Bioinformatics, Astronomy, and Social Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58700fb",
   "metadata": {},
   "source": [
    "Great job on completing the lesson, Voyager! Now, it's time to put your skills to the test.\n",
    "\n",
    "Suppose we have a dataset containing information about different genes, their discovery dates, and their popularity on a particular social network. Specifically, we want to know which gene is most popular on the social network.\n",
    "\n",
    "Your task is to run the given code. This code sorts our data based on its popularity on the social network in descending order, providing us with the information we seek.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7665e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gene Discovery_Date  Popularity_on_Social_Network\n",
      "0  Gene A     1935-02-15                          1250\n",
      "1  Gene B     1935-02-16                           500\n",
      "2  Gene C     1935-02-17                           800\n",
      "3  Gene D     1935-02-18                          2000\n",
      "     Gene Discovery_Date  Popularity_on_Social_Network\n",
      "3  Gene D     1935-02-18                          2000\n",
      "0  Gene A     1935-02-15                          1250\n",
      "2  Gene C     1935-02-17                           800\n",
      "1  Gene B     1935-02-16                           500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gene dataset with discovery dates and its popularity on a specific social network\n",
    "data = {\n",
    "    \"Gene\": [\"Gene A\", \"Gene B\", \"Gene C\", \"Gene D\"],\n",
    "    \"Discovery_Date\": pd.date_range('02/15/1935', periods=4),\n",
    "    \"Popularity_on_Social_Network\": np.array([1250, 500, 800, 2000])\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_gene_info = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_gene_info)\n",
    "\n",
    "# Suppose we need to sort the data based on its popularity on the social network\n",
    "df_gene_info_sorted = df_gene_info.sort_values(by='Popularity_on_Social_Network', ascending=False)\n",
    "\n",
    "# Print the DataFrame after sorting\n",
    "print(df_gene_info_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65dd95",
   "metadata": {},
   "source": [
    "Well done, Stargazer! You have successfully performed some incredible data manipulations on both bioinformatics and astronomical datasets. Now, it would be interesting to explore how data manipulation differs when dealing with lowercase DNA sequences.\n",
    "\n",
    "So, here's your chance! In the provided starter code, modify it so the code works for DNA sequences written in lowercase. Prepare to dive deep into the nucleotide world. However, remember that uppercase letters are not allowed this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6b89d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bioinformatics DataFrame:\n",
      "     Gene Sequence Reverse_Sequence\n",
      "0  Gene M  tgccgta          atgccgt\n",
      "1  Gene N  aatgcgt          tgcgtaa\n",
      "2  Gene O  cgtacgt          tgcatgc\n",
      "3  Gene P  ggctatg          gtatcgg\n",
      "\n",
      "Filtered Astronomical DataFrame:\n",
      "   Star_ID  Right_Ascension  Declination Observation_Date\n",
      "3        4             48.1          8.9       2020-07-04\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a dataset simulating bioinformatics data, now with lowercase sequences only\n",
    "data_bio = {\n",
    "    \"Gene\": [\"Gene M\", \"Gene N\", \"Gene O\", \"Gene P\"],\n",
    "    \"Sequence\": [\"tgccgta\", \"aatgcgt\", \"cgtacgt\", \"ggctatg\"]  # all lowercase\n",
    "}\n",
    "df_bio = pd.DataFrame(data_bio)\n",
    "\n",
    "# Ensure all sequences are lowercase (in case of input from mixed sources)\n",
    "df_bio['Sequence'] = df_bio['Sequence'].str.lower()\n",
    "\n",
    "# Reverse the lowercase DNA Sequence\n",
    "df_bio['Reverse_Sequence'] = df_bio['Sequence'].apply(lambda x: x[::-1])\n",
    "\n",
    "# Creating a dataset simulating astronomical star data\n",
    "data_astro = {\n",
    "    \"Star_ID\": np.arange(1, 5),\n",
    "    \"Right_Ascension\": [210.50, 68.80, 310.70, 48.1],\n",
    "    \"Declination\": [-32.85, 40.08, -16.82, 8.90],\n",
    "    \"Observation_Date\": pd.date_range('07/01/2020', periods=4)\n",
    "}\n",
    "df_astro = pd.DataFrame(data_astro)\n",
    "\n",
    "# Filtering out stars observed after a specific date\n",
    "filter_date = pd.to_datetime('2020-07-03')\n",
    "df_astro_filtered = df_astro[df_astro['Observation_Date'] > filter_date]\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"Bioinformatics DataFrame:\")\n",
    "print(df_bio)\n",
    "\n",
    "print(\"\\nFiltered Astronomical DataFrame:\")\n",
    "print(df_astro_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429b1aa",
   "metadata": {},
   "source": [
    "Fantastic job, navigator! You're conquering this data cosmos. Here's another challenge.\n",
    "\n",
    "You have seen how to compute the length of gene sequences in bioinformatics and filter stars in astronomical data based on the date of observation. Let's mix it up!\n",
    "\n",
    "Given bioinformatics data, can you compute the length of each reversed gene sequence? Additionally, can you filter the astronomical star data based on a specified date using the data from the reverse sequence computation?\n",
    "\n",
    "Let's rock it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39459b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Gene  Sequence  Popularity_on_Social_Network  Reverse_Sequence_Length\n",
      "0  Gene I  AGCTTCGA                           130                        8\n",
      "1  Gene J  TGAATCGC                           520                        8\n",
      "2  Gene K  CTGACGTA                            85                        8\n",
      "3  Gene L  CGTAATGC                           200                        8\n",
      "   Star_ID  Right_Ascension  Declination  Magnitude Observation_Date\n",
      "0        1            205.3        -30.8       2.04       2020-06-01\n",
      "1        2             64.9         39.1       1.25       2020-06-02\n",
      "2        3            305.8        -15.8       3.17       2020-06-03\n",
      "3        4             46.5          8.3       1.90       2020-06-04\n",
      "   Star_ID  Right_Ascension  Declination  Magnitude Observation_Date\n",
      "0        1            205.3        -30.8       2.04       2020-06-01\n",
      "1        2             64.9         39.1       1.25       2020-06-02\n",
      "2        3            305.8        -15.8       3.17       2020-06-03\n",
      "3        4             46.5          8.3       1.90       2020-06-04\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a data set simulating bioinformatics data\n",
    "data_bio = {\n",
    "    \"Gene\": [\"Gene I\", \"Gene J\", \"Gene K\", \"Gene L\"],\n",
    "    \"Sequence\": [\"AGCTTCGA\", \"TGAATCGC\", \"CTGACGTA\", \"CGTAATGC\"],\n",
    "    \"Popularity_on_Social_Network\": np.array([130, 520, 85, 200])\n",
    "}\n",
    "df_bio = pd.DataFrame(data_bio)\n",
    "\n",
    "# Compute the length of each reversed DNA sequence and add it to a new column, \"Reverse_Sequence_Length\"\n",
    "df_bio['Reverse_Sequence_Length'] = df_bio['Sequence'].apply(lambda x: len(x[::-1]))\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_bio)\n",
    "\n",
    "# Creating a data set simulating astronomical star data\n",
    "data_astro = {\n",
    "    \"Star_ID\": np.arange(1, 5),\n",
    "    \"Right_Ascension\": [205.30, 64.90, 305.80, 46.5],\n",
    "    \"Declination\": [-30.80, 39.10, -15.80, 8.30],\n",
    "    \"Magnitude\": [2.04, 1.25, 3.17, 1.9],\n",
    "    \"Observation_Date\": pd.date_range('06/01/2020', periods=4)\n",
    "}\n",
    "df_astro = pd.DataFrame(data_astro)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_astro)\n",
    "\n",
    "# Filter out stars observed after a specific date\n",
    "filter_date = pd.to_datetime('2020-01-02')\n",
    "df_astro_filtered = df_astro[df_astro['Observation_Date'] > filter_date]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df_astro_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2205d1b",
   "metadata": {},
   "source": [
    "Champion! You've made it to the last stride of this thrilling journey. Now it's time to go solo and build the final code from scratch! You have bioinformatics data regarding specific genes and astronomical data about certain stars. Your mission, should you accept it, is to manipulate gene sequences, and filter the star observations based on a specific date.\n",
    "\n",
    "Look out for the TODO statements in the starter code and use your mastery of Python, Pandas, and Numpy to achieve this. Ready for your solo flight? Let's jump!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7c45d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ACTGAC\n",
      "1    TAGCAG\n",
      "2    GATCGT\n",
      "3    TCAGTA\n",
      "Name: reverse, dtype: object\n",
      "   Star_ID  Right_Ascension  Declination  Magnitude Observation_Date\n",
      "3        4             46.5          8.3        1.9       2020-06-04\n",
      "1    500\n",
      "3    200\n",
      "0    125\n",
      "2     85\n",
      "Name: Popularity_on_Social_Network, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a data set simulating bioinformatics data\n",
    "data_bio = {\n",
    "    \"Gene\": [\"HP\", \"MPZ\", \"PMP22\", \"GJB1\"],\n",
    "    \"Sequence\": [\"CAGTCA\", \"GACGAT\", \"TGCTAG\", \"ATGACT\"],\n",
    "    \"Discovery_Date\": pd.date_range('01/01/1950', periods=4),\n",
    "    \"Popularity_on_Social_Network\": np.array([125, 500, 85, 200])\n",
    "}\n",
    "df_bio = pd.DataFrame(data_bio)\n",
    "\n",
    "# TODO: Reverse the 'Sequence' entries\n",
    "df_bio['reverse'] = df_bio[\"Sequence\"].apply(lambda x: x[::-1])\n",
    "# Creating a data set simulating astronomical star data\n",
    "data_astro = {\n",
    "    \"Star_ID\": np.arange(1, 5),\n",
    "    \"Right_Ascension\": [205.30, 64.90, 305.80, 46.5],\n",
    "    \"Declination\": [-30.80, 39.10, -15.80, 8.30],\n",
    "    \"Magnitude\": [2.04, 1.25, 3.17, 1.9],\n",
    "    \"Observation_Date\": pd.date_range('06/01/2020', periods=4)\n",
    "}\n",
    "df_astro = pd.DataFrame(data_astro)\n",
    "filter_date = pd.to_datetime('06/03/2020')\n",
    "\n",
    "# TODO: Filter based on the 'Observation_Date'\n",
    "df_astro_filter = df_astro[df_astro[\"Observation_Date\"] > filter_date]\n",
    "# TODO: Find the most popular gene\n",
    "df_bio_popular = df_bio.sort_values(by='Popularity_on_Social_Network', ascending=False)\n",
    "# TODO: Output the data for bioinformatics (with reversed sequence), filtered star data, and the most popular gene\n",
    "print(df_bio['reverse'])\n",
    "print(df_astro_filter)\n",
    "print(df_bio_popular['Popularity_on_Social_Network'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d15b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c2b288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
